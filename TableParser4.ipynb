{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DocX - TABLE Parser\n",
    "#Infers a table with arbitrary number of columns from reoccuring patterns in text lines\n",
    "#(c) Alexander Hirner 2016, no redistribution without permission\n",
    "\n",
    "#Main assumptions Table identificatin:\n",
    "#1) each row is either in one line or not a row at all\n",
    "#2) each column features at least one number (=dollar amount)\n",
    "#2a) each column features at least one date-like string [for time-series only]\n",
    "#3) a table exists if rows are in narrow consecutive order and share similarities --> scoring algo [DONE] \n",
    "#4) each column is separated by more than x consecutive whitespace indicators (e.g. '  ' or '..')\n",
    "\n",
    "#Feature List Todo:\n",
    "#1) Acknowledge footnotes / make lower meta-data available\n",
    "#2) make delimiter length smartly dependent on number of columns (possible iterative approach)\n",
    "#3) improve captioning: expand non canonical values in tables [DONE] .. but not to the extent how types match up  --> use this to further\n",
    "## delineate between caption and headers\n",
    "#4) UI: parameterize extraction on the show page on the fly\n",
    "#5) deeper type inference on token level: type complex [DONE], subtype header (centered, capitalized), \n",
    "## subtype page nr., type free flow [DONE, need paragraph]\n",
    "#5a) re\n",
    "#6) Respect negative values with potential '-' for numerical values\n",
    "#7)\n",
    "#8) classify tables with keywords (Muni Bonds) and unsupervised clustering (Hackathon)\n",
    "#9) Restructure folder and URI around MD5 hash (http://stackoverflow.com/questions/24570066/calculate-md5-from-werkzeug-datastructures-filestorage-without-saving-the-object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "config = { \"min_delimiter_length\" : 4, \"min_columns\": 2, \"min_consecutive_rows\" : 3, \"max_grace_rows\" : 4,\n",
    "          \"caption_assign_tolerance\" : 10.0, \"meta_info_lines_above\" : 8, \"threshold_caption_extension\" : 0.45,\n",
    "         \"header_good_candidate_length\" : 3, \"complex_leftover_threshold\" : 2, \"min_canonical_rows\" : 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask, request, redirect, url_for, send_from_directory\n",
    "from werkzeug import secure_filename\n",
    "from flask import jsonify, render_template, make_response\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Tag ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regex tester online: https://regex101.com\n",
    "#Contrast with Basic table parsing capabilities of http://docs.astropy.org/en/latest/io/ascii/index.html\n",
    "\n",
    "tokenize_pattern = ur\"[.]{%i,}|[\\ \\$]{%i,}|\" % ((config['min_delimiter_length'],)*2)\n",
    "tokenize_pattern = ur\"[.\\ \\$]{%i,}\" % (config['min_delimiter_length'],)\n",
    "footnote_inidicator = ur\"[^,_!a-zA-Z0-9.]\"\n",
    "\n",
    "column_pattern = OrderedDict()\n",
    "#column_pattern['large_num'] = ur\"\\d{1,3}(,\\d{3})*(\\.\\d+)?\"\n",
    "column_pattern['large_num'] = ur\"(([0-9]{1,3})(,\\d{3})+(\\.[0-9]{2})?)\"\n",
    "column_pattern['small_float'] = ur\"[0-9]+\\.[0-9]+\"\n",
    "column_pattern['integer'] = ur\"^\\s*[0-9]+\\s*$\"\n",
    "#column_patter['delimiter'] = \"[_=]{6,}\"\n",
    "#column_pattern['other'] = ur\"([a-zA-Z0-9]{2,}\\w)\"\n",
    "column_pattern['other'] = ur\".+\"\n",
    "\n",
    "subtype_indicator = OrderedDict()\n",
    "subtype_indicator['dollar'] = ur\".*\\$.*\"\n",
    "subtype_indicator['rate'] = ur\"[%]\"\n",
    "#enter full set of date patterns here if we want refinement early on\n",
    "subtype_indicator['year'] = ur\"(20[0-9]{2})|(19[0-9]{2})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import dateutil.parser as date_parser\n",
    "#Implement footnote from levtovers\n",
    "def tag_token(token, ws):\n",
    "    for t, p in column_pattern.iteritems():\n",
    "        result = re.search(p, token)\n",
    "        if result:\n",
    "            leftover = token[:result.start()], token[result.end():]\n",
    "            lr = \"\".join(leftover)\n",
    "            value = token[result.start():result.end()]\n",
    "            \n",
    "            if len(lr) >= config['complex_leftover_threshold']:\n",
    "                return \"complex\", \"unknown\", token, leftover\n",
    "            \n",
    "            subtype = \"none\"\n",
    "            #First match on left-overs\n",
    "            for sub, indicator in subtype_indicator.iteritems():\n",
    "                if re.match(indicator, lr): subtype = sub\n",
    "            #Only if no indicator matched there, try on full token\n",
    "            if subtype == \"none\":\n",
    "                for sub, indicator in subtype_indicator.iteritems():\n",
    "                    if re.match(indicator, token): subtype = sub\n",
    "            #Only if no indicator matched again, try on whitespace\n",
    "            if subtype == \"none\":\n",
    "                for sub, indicator in subtype_indicator.iteritems():\n",
    "                    if re.match(indicator, ws): subtype = sub\n",
    "            #print token, \":\", ws, \":\", subtype\n",
    "                        \n",
    "            return t, subtype, value, leftover\n",
    "    return \"unknown\", \"none\", token, \"\"\n",
    "    \n",
    "def row_feature(line):\n",
    "    matches = re.finditer(tokenize_pattern, line)\n",
    "    start_end = [ (match.start(), match.end()) for match in matches]\n",
    "    #No delimiter found so it's free flow text\n",
    "    if len(start_end) < 1:\n",
    "        if len(line) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            return [{'start' : 0, 'value' : line, 'type' : 'freeform', 'subtype' : 'none'}]\n",
    "    \n",
    "    tokens = re.split(tokenize_pattern, line)\n",
    "    if tokens[0] == \"\": \n",
    "        tokens = tokens[1:]\n",
    "    else:\n",
    "        start_end = [(0,0)] + start_end\n",
    "    \n",
    "    features = []\n",
    "    for se, token in zip(start_end, tokens):\n",
    "        t, subtype, value, leftover = tag_token(token, line[se[0]:se[1]])\n",
    "        feature = {\"start\" : se[1], \"value\" : value, \"type\" : t, \"subtype\" : subtype, \"leftover\" : leftover}\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Establish whether amount of rows is above a certain threshold and whether there is at least one number\n",
    "def row_qualifies(row):\n",
    "    return len(row) >= config['min_columns'] and sum( 1 if c['type'] in ['large_num', 'small_float', 'integer'] else 0 for c in row) > 0\n",
    "\n",
    "def row_equal_types(row1, row2):\n",
    "    same_types = sum (map(lambda t: 1 if t[0]==t[1] else 0, ((c1['type'], c2['type']) for c1, c2 in zip(row1, row2))))\n",
    "    return same_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Non qualified rows arm for consistency check but are tolerated for max_grace_rows (whitespace, breakline, junk)\n",
    "def filter_row_spans_new(row_features, row_qualifies=row_qualifies, ):    \n",
    "\n",
    "    min_consecutive = config[\"min_consecutive_rows\"]\n",
    "    grace_rows = config['max_grace_rows']\n",
    "\n",
    "    last_qualified = None    \n",
    "    consecutive = 0\n",
    "    underqualified = 0\n",
    "    consistency_check = False\n",
    "    i = 0\n",
    "    \n",
    "    for j, row in enumerate(row_features):\n",
    "        qualifies = row_qualifies(row)\n",
    "        if consistency_check:\n",
    "            print \"BENCHMARKING %s AGAINST:\" % row_to_string(row), row_to_string(row_features[last_qualified], 'type')\n",
    "            if not row_type_compatible(row_features[last_qualified], row):\n",
    "                qualifies = False\n",
    "            consistency_check = False\n",
    "        #print qualifies, row_to_string(row)\n",
    "        \n",
    "        if qualifies:\n",
    "            if last_qualified is None:\n",
    "                last_qualified = i\n",
    "                consecutive = 1\n",
    "            else:\n",
    "                consecutive += 1    \n",
    "        else:\n",
    "            underqualified += 1\n",
    "            if underqualified > grace_rows:\n",
    "                if consecutive >= min_consecutive:\n",
    "                    #TODO: do post splitting upon type check and benchmark\n",
    "                    print \"YIELDED from\", last_qualified, \"to\", i-underqualified+1\n",
    "                    yield last_qualified, i-underqualified+1\n",
    "\n",
    "                last_qualified = None                \n",
    "                consecutive = 0\n",
    "                underqualified = 0\n",
    "                consistency_check = False\n",
    "            else:\n",
    "                if last_qualified: \n",
    "                    consistency_check = True\n",
    "        #print i, last_qualified, consecutive, consistency_check, row_to_string(row)\n",
    "        i += 1\n",
    "        \n",
    "    if consecutive >= min_consecutive:\n",
    "        yield last_qualified, i-underqualified\n",
    "        \n",
    "def row_to_string(row, key='value', sep='|'):\n",
    "    return sep.join(c[key] for c in row)\n",
    "\n",
    "def row_type_compatible(row_canonical, row_test):\n",
    "    #Test whether to break because types differ too much\n",
    "    no_fit = 0\n",
    "    for c in row_test:\n",
    "        dist = (abs(c['start']-lc['start']) for lc in row_canonical)\n",
    "        val, idx = min((val, idx) for (idx, val) in enumerate(dist))\n",
    "        if c['type'] != row_canonical[idx]['type']:\n",
    "            no_fit += 1\n",
    "\n",
    "    fraction_no_fit = no_fit / float(len(row_test))\n",
    "    #print \"test row\", row_to_string(row_test), \") against types (\", row_to_string(row_canonical, 'type'), \") has %f unmatching types\" % fraction_no_fit    \n",
    "    return fraction_no_fit < config[\"threshold_caption_extension\"]\n",
    "\n",
    "def filter_row_spans(row_features, row_qualifies):    \n",
    "\n",
    "    min_consecutive = config[\"min_consecutive_rows\"]\n",
    "    grace_rows = config['max_grace_rows']\n",
    "\n",
    "    last_qualified = None    \n",
    "    consecutive = 0\n",
    "    underqualified = 0\n",
    "    underqualified_rows = [] #Tuples of row number and the row    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for j, row in enumerate(row_features):\n",
    "        if row_qualifies(row):\n",
    "            underqualified = 0\n",
    "            if last_qualified is None:\n",
    "                last_qualified = i\n",
    "                consecutive = 1\n",
    "            else:\n",
    "                consecutive += 1    \n",
    "        else:\n",
    "            underqualified += 1\n",
    "            underqualified_rows.append((j, row) )\n",
    "            if underqualified > grace_rows:\n",
    "                if consecutive >= min_consecutive:\n",
    "                    yield last_qualified, i-underqualified+1\n",
    "\n",
    "                last_qualified = None\n",
    "                consecutive = 0\n",
    "                underqualified = 0\n",
    "        #print i, underqualified, last_qualified, consecutive#, \"\" or row\n",
    "        i += 1\n",
    "        \n",
    "    if consecutive >= min_consecutive:\n",
    "        yield last_qualified, i-underqualified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def row_to_string(row, key='value', sep='|'):\n",
    "    return sep.join(c[key] for c in row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readjust_cols(feature_row, slots):\n",
    "\n",
    "    feature_new = [{'value' : 'NaN'}] * len(slots)\n",
    "    for v in feature_row:\n",
    "        dist = (abs((float(v['start'])) - s) for s in slots)\n",
    "        val , idx = min((val, idx) for (idx, val) in enumerate(dist))\n",
    "        if val <= config['caption_assign_tolerance']: feature_new[idx] = v\n",
    "\n",
    "    return feature_new\n",
    "\n",
    "\n",
    "def normalize_rows(rows_in, structure):\n",
    "    slots = [c['start'] for c in structure] \n",
    "    nrcols = len(structure)\n",
    "    \n",
    "    for r in rows_in:\n",
    "        if len(r) != nrcols:\n",
    "            if len(r)/float(nrcols) > config['threshold_caption_extension']:          \n",
    "                yield readjust_cols(r, slots)\n",
    "        else:\n",
    "            yield r\n",
    "\n",
    "#TODO: make side-effect free\n",
    "def structure_rows(row_features, meta_features):\n",
    "    #Determine maximum nr. of columns\n",
    "    lengths = Counter(len(r) for r in row_features)\n",
    "    nrcols = config['min_columns']\n",
    "    for l in sorted(lengths.keys(), reverse=True):\n",
    "        nr_of_l_rows = lengths[l]\n",
    "        if nr_of_l_rows/float(len(row_features)) > config['min_canonical_rows']:\n",
    "            nrcols = l\n",
    "            break\n",
    "            \n",
    "    canonical = filter(lambda r: len(r) == nrcols , row_features)\n",
    "    \n",
    "    #for c in canonical: print len(c), row_to_string(c)\n",
    "        \n",
    "    structure = []\n",
    "    for i in range(nrcols):\n",
    "        col = {}\n",
    "        col['start'] = float (sum (c[i]['start'] for c in canonical )) / len(canonical)\n",
    "    \n",
    "        types = Counter(c[i]['type'] for c in canonical)\n",
    "        col['type'] = types.most_common(1)[0][0]\n",
    "        subtypes = Counter(c[i]['subtype'] for c in canonical if c[i]['subtype'] is not \"none\")        \n",
    "        subtype = \"none\" if len(subtypes) == 0 else subtypes.most_common(1)[0][0]\n",
    "        col['subtype'] = subtype\n",
    "        structure.append(col)\n",
    "\n",
    "    #Test how far up the types are compatible and by that are data vs caption\n",
    "    for r in row_features:\n",
    "        #if r in canonical:\n",
    "        if len(r) and row_type_compatible(structure, r):\n",
    "            break\n",
    "        else:\n",
    "            meta_features.append(r)\n",
    "            row_features.remove(r)\n",
    "     \n",
    "    meta_features.reverse()\n",
    "    #for m in meta_features: print \"META\", row_to_string(m)\n",
    " \n",
    "    captions = [''] * nrcols\n",
    "    single_headers = []\n",
    "    latest_caption_len = 1\n",
    "    slots = [c['start'] for c in structure] \n",
    "    for mf in meta_features:\n",
    "        #if we have at least two tokens in the line, consider them forming captions\n",
    "        nr_meta_tokens = len(mf)\n",
    "        if nr_meta_tokens > 1 and nr_meta_tokens >= latest_caption_len:\n",
    "            #Find closest match: TODO = allow doubling of captions if it is centered around more than one and len(mf) is at least half of nrcols\n",
    "            for c in mf:\n",
    "                dist = (abs((float(c['start'])) - s) for s in slots)\n",
    "                val, idx = min((val, idx) for (idx, val) in enumerate(dist))\n",
    "                if val <= config['caption_assign_tolerance']: \n",
    "                    captions[idx] = c['value'] + ' ' + captions[idx]\n",
    "                else: single_headers.append(c['value'])\n",
    "            #latest_caption_len = nr_meta_tokens\n",
    "        #otherwise, throw them into headers directly for now                                                                           \n",
    "        else:\n",
    "            #Only use single tokens to become headers, throw others away\n",
    "            if len(mf) == 1 and mf[0]['type'] != 'freeform': single_headers.append(mf[0]['value'])\n",
    "    \n",
    "\n",
    "    #Assign captions as the value in structure\n",
    "    for i, c in enumerate(captions):\n",
    "        structure[i]['value'] = c\n",
    "    #Expand all the non canonical rows with NaN values (Todo: if types are very similar)\n",
    "    normalized_data = [r for r in normalize_rows(row_features, structure)]            \n",
    "    \n",
    "    return structure, normalized_data, single_headers\n",
    "\n",
    "\n",
    "def convert_to_table(rows, b, e, above):\n",
    "    table = {'begin_line' : b, 'end_line' : e}\n",
    "\n",
    "    data_rows = rows[b:e]\n",
    "    meta_rows = rows[b-above:b]\n",
    "\n",
    "    structure, data, headers = structure_rows(data_rows, meta_rows)\n",
    "\n",
    "    captions = [(col['value'] if 'value' in col.keys() else \"---\") +\"\\n(%s, %s)\" % (col['type'], col['subtype']) for col in structure]\n",
    "    table['captions'] = captions\n",
    "    table['data'] = data           \n",
    "    table['header'] = \" | \".join(headers)\n",
    "\n",
    "    return table \n",
    "\n",
    "def indexed_tables_from_rows(row_features):\n",
    "    \n",
    "    #Uniquely identify tables by their first row\n",
    "    tables = OrderedDict()\n",
    "    last_end = 0\n",
    "    for b,e in filter_row_spans(row_features, row_qualifies):\n",
    "        #Slice out the next table and limit the context rows to have no overlaps\n",
    "        #Todo: manage the lower meta lines\n",
    "        tables[b] = convert_to_table(row_features, b, e, min(config['meta_info_lines_above'], b - last_end))\n",
    "        last_end = tables[b]['end_line']\n",
    "    return tables    \n",
    "    \n",
    "def return_tables(txt_path):\n",
    "    \n",
    "    #Uniquely identify tables by their first row\n",
    "    tables = OrderedDict()\n",
    "    \n",
    "    with codecs.open(txt_path, \"r\", \"utf-8\") as f:\n",
    "        lines = [l.replace(u'\\n', '').replace(u'\\r', '') for l in f]\n",
    "        rows = [row_feature(l) for l in lines] \n",
    "        \n",
    "        return indexed_tables_from_rows(rows)\n",
    "\n",
    "def table_to_df(table):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(table['captions'])):\n",
    "        values = []\n",
    "        for r in table['data']:\n",
    "            values.append(r[i]['value'])\n",
    "        df[i] = values\n",
    "    df.columns = table['captions']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web App ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TITLE = \"TabulaRazr (docX)\"\n",
    "\n",
    "scripts = []\n",
    "css = [\n",
    "    \"./bower_components/bootstrap/dist/css/bootstrap.min.css\",\n",
    "    \"./css/main.css\",\n",
    "    \"./css/style.css\"\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "UPLOAD_FOLDER = './static/ug'\n",
    "ALLOWED_EXTENSIONS = set(['txt', 'pdf'])\n",
    "\n",
    "TITLE = \"TabulaRazr\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "def get_extension(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1] \n",
    "\n",
    "def allowed_file(filename):\n",
    "    return get_extension(filename) in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        file = request.files['file']\n",
    "        project = request.form['project']\n",
    "        \n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            extension = get_extension(file.filename)\n",
    "            path = os.path.join(app.config['UPLOAD_FOLDER'], project, filename)\n",
    "            \n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], project, filename))\n",
    "            \n",
    "            if extension == \"pdf\":\n",
    "                txt_path = path+'.txt'\n",
    "                filename += '.txt'        \n",
    "                if not os.path.isfile(txt_path):\n",
    "                    #Layout preservation crucial to preserve clues about tabular data\n",
    "                    cmd = \"pdftotext -enc UTF-8 -layout %s %s \" % (path, txt_path)\n",
    "                    os.system(cmd)            \n",
    "\n",
    "            return redirect(url_for('analyze', filename=filename, project=project))\n",
    "\n",
    "    return render_template('index.html',\n",
    "        title=TITLE ,\n",
    "        css=css)\n",
    "\n",
    "@app.route('/analyze/<filename>', methods=['GET', 'POST'])\n",
    "def analyze(filename):   \n",
    "\n",
    "    project = request.args.get('project')\n",
    "    txt_path = os.path.join(app.config['UPLOAD_FOLDER'], project, filename)\n",
    "    \n",
    "    if not os.path.isfile(txt_path):\n",
    "        return {'error' : txt_path+' not found' }\n",
    "    \n",
    "    tables = return_tables(txt_path)\n",
    "    \n",
    "    #Export tables\n",
    "    with codecs.open(txt_path + '.tables.json', 'w', \"utf-8\") as file:\n",
    "        json.dump(tables, file)\n",
    "\n",
    "    #Export chart\n",
    "    lines_per_page = 80\n",
    "    nr_data_rows = []\n",
    "    #for t in tables.values():\n",
    "    #    print t\n",
    "    for key, t in tables.iteritems():\n",
    "        e = t['end_line']\n",
    "        b = t['begin_line']\n",
    "        for l in range(b, e):\n",
    "            page = l / lines_per_page\n",
    "            if len(nr_data_rows) <= page:\n",
    "                nr_data_rows += ([0]*(page-len(nr_data_rows)+1))\n",
    "            nr_data_rows[page] += 1\n",
    "    dr = pd.DataFrame()\n",
    "    dr['value'] = nr_data_rows\n",
    "    dr['page'] = range(0, len(dr))\n",
    "    \n",
    "    #plot the row density\n",
    "    chart = filename+\".png\"\n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1, figsize=(8,3) )  # create figure & 1 axis\n",
    "    ax.set_xlabel('page nr.')\n",
    "    ax.set_ylabel('number of data rows')\n",
    "    ax.set_title('Distribution of Rows with Data')\n",
    "    ax.plot(dr['page'], dr['value'], )\n",
    "    fig.savefig(txt_path + '.png')   # save the figure to file\n",
    "    plt.close(fig)                      # close the figure\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        return json.dumps(tables)\n",
    "    \n",
    "    return redirect(url_for('uploaded_file', filename=filename, project=project))\n",
    "    \n",
    "\n",
    "@app.route('/show/<filename>')\n",
    "def uploaded_file(filename):\n",
    "\n",
    "    project = request.args.get('project')    \n",
    "    path = os.path.join(app.config['UPLOAD_FOLDER'], project, filename)\n",
    "    \n",
    "    tables_path = path + '.tables.json'\n",
    "    chart_path = path+\".png\"\n",
    "    \n",
    "    if not os.path.isfile(tables_path):\n",
    "        analyze(path)\n",
    "\n",
    "    with codecs.open(tables_path) as file:\n",
    "        tables = json.load(file)   \n",
    "\n",
    "    #Create HTML\n",
    "    notices = ['Extraction Results for ' + filename, 'Ordered by lines']    \n",
    "    dfs = (table_to_df(table).to_html() for table in tables.values())\n",
    "    headers = []\n",
    "    for t in tables.values():\n",
    "        if 'header' in t:\n",
    "            headers.append(t['header'])\n",
    "        else:\n",
    "            headers.append('-')\n",
    "    meta_data = [{'begin_line' : t['begin_line'], 'end_line' : t['end_line']} for t in tables.values()]\n",
    "\n",
    "    return render_template('viewer.html',\n",
    "        title=TITLE + ' - ' + filename,\n",
    "        base_scripts=scripts, filename=filename, project=project,\n",
    "        css=css, notices = notices, tables = dfs, headers=headers, meta_data=meta_data, chart=chart_path)\n",
    "\n",
    "@app.route('/inspector/<filename>')\n",
    "def inspector(filename):\n",
    "    extension = 'txt'\n",
    "    path = os.path.join(app.config['UPLOAD_FOLDER'], extension, filename)\n",
    "    begin_line = int(request.args.get('data_begin'))\n",
    "    end_line = int(request.args.get('data_end'))\n",
    "    margin_top = config[\"meta_info_lines_above\"]\n",
    "    margin_bottom = margin_top\n",
    "    \n",
    "    notices = ['showing data lines from %i to %i with %i meta-lines above and below' % (begin_line, end_line, margin_top)]\n",
    "    with codecs.open(path, \"r\", \"utf-8\") as file:\n",
    "        lines = [l.encode('utf-8') for l in file][begin_line - margin_top:end_line + margin_bottom]\n",
    "        top_lines = lines[:margin_top]\n",
    "        table_lines = lines[margin_top:margin_top+end_line-begin_line]\n",
    "        bottom_lines = lines[margin_top+end_line-begin_line:]\n",
    "    \n",
    "    offset = begin_line-margin_top\n",
    "    table_id = begin_line\n",
    "    \n",
    "    return render_template('inspector.html',\n",
    "        title=TITLE,\n",
    "        base_scripts=scripts, css=css, notices = notices, filename=filename, top_lines=top_lines, \n",
    "        table_lines=table_lines, bottom_lines=bottom_lines, offset=offset, table_id=begin_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Exception on /show/EA716610-EA562590-EA958701.pdf.txt [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-43-e7c38584e634>\", line 135, in uploaded_file\n",
      "    css=css, notices = notices, tables = dfs, headers=headers, meta_data=meta_data, chart=chart_path)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/templating.py\", line 128, in render_template\n",
      "    context, ctx.app)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/templating.py\", line 110, in _render\n",
      "    rv = template.render(context)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jinja2/environment.py\", line 989, in render\n",
      "    return self.environment.handle_exception(exc_info, True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jinja2/environment.py\", line 754, in handle_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/home/dociq/docIQ/public_sector_credit/templates/viewer.html\", line 27, in top-level template code\n",
      "    <img src={{url_for('static/..', filename=chart)}}></img>\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/helpers.py\", line 312, in url_for\n",
      "    return appctx.app.handle_url_build_error(error, endpoint, values)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1641, in handle_url_build_error\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/flask/helpers.py\", line 305, in url_for\n",
      "    force_external=external)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/werkzeug/routing.py\", line 1758, in build\n",
      "    raise BuildError(endpoint, values, method, self)\n",
      "BuildError: Could not build url for endpoint 'static/..' with values ['filename']. Did you mean 'static' instead?\n"
     ]
    }
   ],
   "source": [
    "def run_from_ipython():\n",
    "    try:\n",
    "        __IPYTHON__\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "if run_from_ipython():\n",
    "    app.run(host='0.0.0.0', port = 7080) #Borrow Zeppelin port for now\n",
    "else:\n",
    "    PORT = int(os.getenv('PORT', 8000))\n",
    "    app.run(debug=True, host='0.0.0.0', port = PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
